# sankey_traffic_streamlit.py
import pandas as pd
import plotly.graph_objects as go
import logging
import streamlit as st
from datetime import datetime

# ===================== 1. é¡µé¢é…ç½® =====================
st.set_page_config(
    page_title="å¤šç«™ç‚¹æµé‡-é”€é‡æ¡‘åŸºå›¾åˆ†æ",
    page_icon="ğŸŒ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ===================== 2. å…¨å±€é…ç½® =====================
SITE_CONFIG = {
    "Amazon-US": {"cn_name": "äºšé©¬é€Šç¾å›½ç«™", "color": "#87CEEB"},
    "Amazon-JP": {"cn_name": "äºšé©¬é€Šæ—¥æœ¬ç«™", "color": "#FF6B6B"},
    "Amazon-UK": {"cn_name": "äºšé©¬é€Šè‹±å›½ç«™", "color": "#4ECDC4"},
    "Shopify": {"cn_name": "Shopifyç‹¬ç«‹ç«™", "color": "#DDA0DD"}
}

TRAFFIC_ORDER = [
    "Amazonç«™å†…å¹¿å‘Š",   # 1
    "Amazon-DSP",       # 2
    "Amazonè‡ªç„¶æµé‡",   # 3
    "Amazon-FB",        # 4
    "SP-GG",            # 5
    "SP-FB",            # 6
    "SP-è‡ªç„¶",          # 7
    "SP-å…¶ä»–"           # 8
]

TRAFFIC_MAPPING = {
    "Amazonç«™å†…å¹¿å‘Š": {
        "group_id": "ç»„1",
        "site": "Amazon-US",
        "nodes": {
            "exposure": "ç«™å†…æ›å…‰",
            "level2_exposure": "Amazon-USæ›å…‰",
            "click": "ç«™å†…ç‚¹å‡»",
            "level2_click": "Amazon-USç‚¹å‡»",
            "sales": "ç«™å†…é”€é‡",
            "level2_sales": "Amazon-USé”€é‡"
        }
    },
    "Amazon-DSP": {
        "group_id": "ç»„2",
        "site": "Amazon-US",
        "nodes": {
            "exposure": "DSPæ›å…‰",
            "level2_exposure": "Amazon-USæ›å…‰",
            "click": "DSPç‚¹å‡»",
            "level2_click": "Amazon-USç‚¹å‡»",
            "sales": "DSPé”€é‡",
            "level2_sales": "Amazon-USé”€é‡"
        }
    },
    "Amazonè‡ªç„¶æµé‡": {
        "group_id": "ç»„3",
        "site": "Amazon-US",
        "nodes": {
            "exposure": "Amazonè‡ªç„¶æ›å…‰",
            "level2_exposure": "Amazon-USæ›å…‰",
            "click": "Amazonè‡ªç„¶ç‚¹å‡»",
            "level2_click": "Amazon-USç‚¹å‡»",
            "sales": "Amazonè‡ªç„¶é”€é‡",
            "level2_sales": "Amazon-USé”€é‡"
        }
    },
    "Amazon-FB": {
        "group_id": "ç»„4",
        "site": "Amazon-US",
        "nodes": {
            "exposure": "FBæ›å…‰",
            "level2_exposure": "Amazon-USæ›å…‰",
            "click": "FBç‚¹å‡»",
            "level2_click": "Amazon-USç‚¹å‡»",
            "sales": "FBé”€é‡",
            "level2_sales": "Amazon-USé”€é‡"
        }
    },
    "SP-GG": {
        "group_id": "ç»„5",
        "site": "Shopify",
        "nodes": {
            "exposure": "SP-GGæ›å…‰",
            "level2_exposure": "Shopifyæ›å…‰",
            "click": "SP-GGç‚¹å‡»",
            "level2_click": "Shopifyç‚¹å‡»",
            "sales": "SP-GGé”€é‡",
            "level2_sales": "Shopifyé”€é‡"
        }
    },
    "SP-FB": {
        "group_id": "ç»„6",
        "site": "Shopify",
        "nodes": {
            "exposure": "SP-FBæ›å…‰",
            "level2_exposure": "Shopifyæ›å…‰",
            "click": "SP-FBç‚¹å‡»",
            "level2_click": "Shopifyç‚¹å‡»",
            "sales": "SP-FBé”€é‡",
            "level2_sales": "Shopifyé”€é‡"
        }
    },
    "SP-è‡ªç„¶": {
        "group_id": "ç»„7",
        "site": "Shopify",
        "nodes": {
            "exposure": "SP-è‡ªç„¶æ›å…‰",
            "level2_exposure": "Shopifyæ›å…‰",
            "click": "SP-è‡ªç„¶ç‚¹å‡»",
            "level2_click": "Shopifyç‚¹å‡»",
            "sales": "SP-è‡ªç„¶é”€é‡",
            "level2_sales": "Shopifyé”€é‡"
        }
    },
    "SP-å…¶ä»–": {
        "group_id": "ç»„8",
        "site": "Shopify",
        "nodes": {
            "exposure": "SP-å…¶ä»–æ›å…‰",
            "level2_exposure": "Shopifyæ›å…‰",
            "click": "SP-å…¶ä»–ç‚¹å‡»",
            "level2_click": "Shopifyç‚¹å‡»",
            "sales": "SP-å…¶ä»–é”€é‡",
            "level2_sales": "Shopifyé”€é‡"
        }
    }
}

GROUP_COLORS = {
    "ç»„1": "#9290E6",  # ç«™å†…å¹¿å‘Š
    "ç»„2": "#4ECDC4",  # DSP
    "ç»„3": "#45B7D1",  # è‡ªç„¶æµé‡
    "ç»„4": "#96CEB4",  # FB
    "ç»„5": "#FFA726",  # SP-GG
    "ç»„6": "#AB47BC",  # SP-FB
    "ç»„7": "#1C363F",  # SP-è‡ªç„¶
    "ç»„8": "#F00B0B",  # SP-å…¶ä»–
    **{site: SITE_CONFIG[site]["color"] for site in SITE_CONFIG},
    "æ€»èŠ‚ç‚¹": "lightgray"
}

# åŠ¨æ€ç”Ÿæˆlevel2èŠ‚ç‚¹åˆ—è¡¨
LEVEL2_NODES = []
for traffic_type in TRAFFIC_MAPPING:
    cfg = TRAFFIC_MAPPING[traffic_type]
    LEVEL2_NODES.extend([
        cfg["nodes"]["level2_exposure"],
        cfg["nodes"]["level2_click"],
        cfg["nodes"]["level2_sales"]
    ])
LEVEL2_NODES = list(set(LEVEL2_NODES))

# èŠ‚ç‚¹â†’æµé‡ç±»å‹æ˜ å°„
NODE_TO_TRAFFIC = {}
for traffic_type in TRAFFIC_MAPPING:
    cfg = TRAFFIC_MAPPING[traffic_type]
    unique_nodes = [
        traffic_type,
        cfg["nodes"]["exposure"],
        cfg["nodes"]["click"],
        cfg["nodes"]["sales"]
    ]
    for node in unique_nodes:
        NODE_TO_TRAFFIC[node] = traffic_type

# æ— æ•ˆæµé‡ç±»å‹è¿‡æ»¤åˆ—è¡¨
INVALID_TRAFFIC_TYPES = ["Amazon é¡µé¢æ€»ç‚¹å‡»", "æ€»æ›å…‰", "æ€»ç‚¹å‡»", "æ€»é”€é‡"]

# ===================== 3. è¯»å–Excelå‡½æ•° =====================
@st.cache_data
def read_excel_generate_data(excel_path):
    try:
        df = pd.read_excel(excel_path)
        logger.info(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œæ•°æ®è¡Œæ•°ï¼š{len(df)}")
        st.success(f"âœ… æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œæ•°æ®è¡Œæ•°ï¼š{len(df)}")
    except Exception as e:
        logger.error(f"è¯»å–Excelå¤±è´¥ï¼š{str(e)}")
        st.error(f"âŒ è¯»å–Excelå¤±è´¥ï¼š{str(e)}")
        return []
    
    # æ•°æ®é¢„å¤„ç†
    df["æ—¶é—´_str"] = df["æ—¶é—´"].astype(str)
    df["date"] = df["æ—¶é—´_str"].str.split(" ").str[0].str.replace("/", "-")
    df["date"] = df["date"].replace(["nan", "NaT", ""], pd.NaT)
    df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.strftime("%Y-%m-%d")
    
    data_raw = []
    for _, row in df.iterrows():
        if pd.isna(row["date"]):
            continue
        
        traffic_type = row["æµé‡ç±»å‹"]
        if traffic_type in INVALID_TRAFFIC_TYPES:
            logger.debug(f"è¿‡æ»¤æ— æ•ˆæµé‡ç±»å‹ï¼š{traffic_type}")
            continue
        
        if traffic_type not in TRAFFIC_MAPPING:
            logger.warning(f"æœªé…ç½®çš„æµé‡ç±»å‹ï¼š{traffic_type}ï¼ˆå·²è·³è¿‡ï¼‰")
            continue
        
        cfg = TRAFFIC_MAPPING[traffic_type]
        if cfg["site"] not in SITE_CONFIG:
            logger.warning(f"éæ³•ç«™ç‚¹ï¼š{cfg['site']}ï¼ˆæµé‡ç±»å‹ï¼š{traffic_type}ï¼Œå·²è·³è¿‡ï¼‰")
            continue
        
        date = row["date"]
        exposure = pd.to_numeric(row["æ›å…‰"], errors="coerce") if pd.notna(row["æ›å…‰"]) else 0.0
        click = pd.to_numeric(row["ç‚¹å‡»"], errors="coerce") if pd.notna(row["ç‚¹å‡»"]) else 0.0
        sales = pd.to_numeric(row["é”€é‡"], errors="coerce") if pd.notna(row["é”€é‡"]) else 0.0
        
        data_raw.extend([
            [traffic_type, cfg["nodes"]["exposure"], float(exposure), date, cfg["group_id"], traffic_type],
            [cfg["nodes"]["exposure"], cfg["nodes"]["level2_exposure"], float(exposure), date, cfg["group_id"], traffic_type],
            [cfg["nodes"]["level2_exposure"], "æ€»æ›å…‰", float(exposure), date, cfg["group_id"], traffic_type],
            ["æ€»æ›å…‰", cfg["nodes"]["click"], float(click), date, cfg["group_id"], traffic_type],
            [cfg["nodes"]["click"], cfg["nodes"]["level2_click"], float(click), date, cfg["group_id"], traffic_type],
            [cfg["nodes"]["level2_click"], "æ€»ç‚¹å‡»", float(click), date, cfg["group_id"], traffic_type],
            ["æ€»ç‚¹å‡»", cfg["nodes"]["sales"], float(sales), date, cfg["group_id"], traffic_type],
            [cfg["nodes"]["sales"], cfg["nodes"]["level2_sales"], float(sales), date, cfg["group_id"], traffic_type],
            [cfg["nodes"]["level2_sales"], "æ€»é”€é‡", float(sales), date, cfg["group_id"], traffic_type]
        ])
    
    logger.info(f"ç”Ÿæˆé“¾è·¯æ•°æ®æ¡æ•°ï¼š{len(data_raw)}")
    return data_raw

# ===================== 4. åº”ç”¨æ ‡é¢˜ =====================
st.title("ğŸŒ å¤šç«™ç‚¹æµé‡-é”€é‡æ¡‘åŸºå›¾åˆ†æ")
st.markdown("---")

# ===================== 5. ä¾§è¾¹æ æ§åˆ¶é¢æ¿ =====================
with st.sidebar:
    st.header("âš™ï¸ æ§åˆ¶é¢æ¿")
    
    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader("ä¸Šä¼ Excelæ–‡ä»¶", type=["xlsx", "xls"])
    
    # æœç´¢åŒºåŸŸ
    search_keyword = st.text_input(
        "ğŸ” é“¾è·¯æœç´¢ï¼ˆæ”¯æŒç«™ç‚¹/æµé‡ç±»å‹å…³é”®è¯ï¼‰",
        placeholder="è¾“å…¥å…³é”®è¯ï¼ˆå¦‚US/Shopify/DSP/ç«™å†…ï¼‰",
        help="æ”¯æŒç«™ç‚¹ã€æµé‡ç±»å‹å…³é”®è¯æœç´¢"
    )
    
    # æ¸…ç©ºæœç´¢æŒ‰é’®
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæœç´¢", type="secondary", use_container_width=True):
            search_keyword = ""
            st.rerun()
    
    st.markdown("---")
    st.subheader("ğŸ“… æ—¥æœŸèŒƒå›´")
    
    # æ—¥æœŸè¾“å…¥
    default_start = "2026-01-05"
    default_end = "2026-01-19"
    
    col1, col2 = st.columns(2)
    with col1:
        start_date = st.date_input(
            "å¼€å§‹æ—¥æœŸ",
            value=datetime.strptime(default_start, "%Y-%m-%d").date()
        )
    
    with col2:
        end_date = st.date_input(
            "ç»“æŸæ—¥æœŸ",
            value=datetime.strptime(default_end, "%Y-%m-%d").date()
        )
    
    # æ—¥æœŸéªŒè¯
    if start_date > end_date:
        st.warning("âš ï¸ å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸï¼Œå·²è‡ªåŠ¨äº¤æ¢")
        start_date, end_date = end_date, start_date
    
    st.markdown("---")
    st.subheader("ğŸ“ ç¼©æ”¾æ§åˆ¶")
    
    # ç¼©æ”¾ç³»æ•°
    col1, col2 = st.columns(2)
    with col1:
        exposure_scale = st.number_input(
            "æ›å…‰é“¾è·¯ç¼©æ”¾",
            min_value=0.01,
            max_value=10.0,
            value=0.5,
            step=0.05,
            help="è°ƒæ•´æ›å…‰é“¾è·¯çš„å®½åº¦"
        )
    
    with col2:
        later_scale = st.number_input(
            "åç»­é“¾è·¯ç¼©æ”¾",
            min_value=0.01,
            max_value=50.0,
            value=5.0,
            step=1.0,
            help="è°ƒæ•´ç‚¹å‡»å’Œé”€é‡é“¾è·¯çš„å®½åº¦"
        )
    
    st.markdown("---")
    st.info("ğŸ’¡ æç¤ºï¼šç‚¹å‡»å›¾è¡¨èŠ‚ç‚¹å¯ä»¥æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯")

# ===================== 6. æ•°æ®åˆå§‹åŒ– =====================
# ç¡®å®šExcelæ–‡ä»¶è·¯å¾„
if uploaded_file is not None:
    # å¦‚æœæœ‰ä¸Šä¼ çš„æ–‡ä»¶ï¼Œä½¿ç”¨ä¸Šä¼ çš„æ–‡ä»¶
    EXCEL_PATH = uploaded_file
    st.success(f"ğŸ“‚ å·²ä¸Šä¼ æ–‡ä»¶: {uploaded_file.name}")
else:
    # å¦åˆ™ä½¿ç”¨é»˜è®¤æ–‡ä»¶ï¼ˆæœ¬åœ°æµ‹è¯•æ—¶ï¼‰
    EXCEL_PATH = "1.5-1.19æµé‡æ•°æ®ç»Ÿè®¡.xlsx"

# åŠ è½½æ•°æ®
try:
    data_raw = read_excel_generate_data(EXCEL_PATH)
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(data_raw, columns=["source", "target", "value", "date", "group", "traffic_type"])
    df["date"] = pd.to_datetime(df["date"])
    df["value"] = pd.to_numeric(df["value"], errors="coerce").fillna(0.0)
    
    # æ›´æ–°é»˜è®¤æ—¥æœŸèŒƒå›´
    if df["date"].notna().any():
        default_start = df["date"].min().strftime("%Y-%m-%d")
        default_end = df["date"].max().strftime("%Y-%m-%d")
    
    logger.info(f"æœ‰æ•ˆæ—¥æœŸèŒƒå›´ï¼š{default_start} è‡³ {default_end}")
    
except Exception as e:
    st.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
    st.stop()

# ===================== 7. æ•°æ®ç­›é€‰å’Œå¤„ç† =====================
# æ˜¾ç¤ºæ•°æ®æ‘˜è¦
with st.expander("ğŸ“Š æ•°æ®æ‘˜è¦", expanded=True):
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        total_records = len(df)
        st.metric("æ€»è®°å½•æ•°", total_records)
    
    with col2:
        traffic_types = df["traffic_type"].nunique()
        st.metric("æµé‡ç±»å‹æ•°", traffic_types)
    
    with col3:
        total_exposure = df[df["source"].str.contains("æ›å…‰")]["value"].sum()
        st.metric("æ€»æ›å…‰é‡", f"{total_exposure:,.0f}")
    
    with col4:
        total_sales = df[df["source"].str.contains("é”€é‡")]["value"].sum()
        st.metric("æ€»é”€é‡", f"{total_sales:,.0f}")

# æ•°æ®ç­›é€‰èšåˆ
start_date_dt = pd.Timestamp(start_date)
end_date_dt = pd.Timestamp(end_date)

filtered_df = df[(df["date"] >= start_date_dt) & (df["date"] <= end_date_dt)]
aggregated_df = filtered_df.groupby(["source", "target", "group", "traffic_type"], as_index=False)["value"].sum()
aggregated_df = aggregated_df[aggregated_df["value"] > 0]

# ===================== 8. ç”ŸæˆèŠ‚ç‚¹åˆ—è¡¨ =====================
# æ‹†åˆ†æµé‡ç±»å‹ä¸ºAmazonç»„å’ŒShopifyç»„
Amazon_TRAFFIC = [t for t in TRAFFIC_ORDER if TRAFFIC_MAPPING[t]["site"] == "Amazon-US"]
Shopify_TRAFFIC = [t for t in TRAFFIC_ORDER if TRAFFIC_MAPPING[t]["site"] == "Shopify"]

# åˆ†åˆ«ç”ŸæˆAmazonç»„çš„èŠ‚ç‚¹
Amazon_flow_sources = Amazon_TRAFFIC
Amazon_exposure_nodes = [TRAFFIC_MAPPING[t]["nodes"]["exposure"] for t in Amazon_TRAFFIC]
Amazon_level2_exposure = list(set([TRAFFIC_MAPPING[t]["nodes"]["level2_exposure"] for t in Amazon_TRAFFIC]))
Amazon_click_nodes = [TRAFFIC_MAPPING[t]["nodes"]["click"] for t in Amazon_TRAFFIC]
Amazon_level2_click = list(set([TRAFFIC_MAPPING[t]["nodes"]["level2_click"] for t in Amazon_TRAFFIC]))
Amazon_sales_nodes = [TRAFFIC_MAPPING[t]["nodes"]["sales"] for t in Amazon_TRAFFIC]
Amazon_level2_sales = list(set([TRAFFIC_MAPPING[t]["nodes"]["level2_sales"] for t in Amazon_TRAFFIC]))

# åˆ†åˆ«ç”ŸæˆShopifyç»„çš„èŠ‚ç‚¹
Shopify_flow_sources = Shopify_TRAFFIC
Shopify_exposure_nodes = [TRAFFIC_MAPPING[t]["nodes"]["exposure"] for t in Shopify_TRAFFIC]
Shopify_level2_exposure = list(set([TRAFFIC_MAPPING[t]["nodes"]["level2_exposure"] for t in Shopify_TRAFFIC]))
Shopify_click_nodes = [TRAFFIC_MAPPING[t]["nodes"]["click"] for t in Shopify_TRAFFIC]
Shopify_level2_click = list(set([TRAFFIC_MAPPING[t]["nodes"]["level2_click"] for t in Shopify_TRAFFIC]))
Shopify_sales_nodes = [TRAFFIC_MAPPING[t]["nodes"]["sales"] for t in Shopify_TRAFFIC]
Shopify_level2_sales = list(set([TRAFFIC_MAPPING[t]["nodes"]["level2_sales"] for t in Shopify_TRAFFIC]))

# æ€»èŠ‚ç‚¹ï¼ˆæ›å…‰/ç‚¹å‡»/é”€é‡ï¼‰
total_nodes = ["æ€»æ›å…‰", "æ€»ç‚¹å‡»", "æ€»é”€é‡"]

# æ‹¼æ¥èŠ‚ç‚¹åˆ—è¡¨ï¼šå…ˆAmazonç»„ï¼Œå†Shopifyç»„ï¼ˆç¡®ä¿Shopifyåœ¨ä¸‹æ–¹ï¼‰
all_nodes = (
    # Amazonç»„èŠ‚ç‚¹
    Amazon_flow_sources + Amazon_exposure_nodes + Amazon_level2_exposure + 
    # æ€»æ›å…‰
    total_nodes[:1] + 
    # Amazonç‚¹å‡»ç›¸å…³èŠ‚ç‚¹
    Amazon_click_nodes + Amazon_level2_click + 
    # æ€»ç‚¹å‡»
    total_nodes[1:2] + 
    # Amazoné”€é‡ç›¸å…³èŠ‚ç‚¹
    Amazon_sales_nodes + Amazon_level2_sales + 
    # Shopifyç»„èŠ‚ç‚¹ï¼ˆæ”¾åˆ°Amazonä¹‹åï¼Œæ˜¾ç¤ºåœ¨ä¸‹æ–¹ï¼‰
    Shopify_flow_sources + Shopify_exposure_nodes + Shopify_level2_exposure + 
    # Shopifyç‚¹å‡»ç›¸å…³èŠ‚ç‚¹
    Shopify_click_nodes + Shopify_level2_click + 
    # Shopifyé”€é‡ç›¸å…³èŠ‚ç‚¹
    Shopify_sales_nodes + Shopify_level2_sales + 
    # æ€»é”€é‡
    total_nodes[2:]
)

node_ids = {node: idx for idx, node in enumerate(all_nodes)}

# ===================== 9. èŠ‚ç‚¹ç»Ÿè®¡ =====================
node_stats = {}
for node in all_nodes:
    incoming = aggregated_df[aggregated_df["target"] == node]["value"].sum()
    outgoing = aggregated_df[aggregated_df["source"] == node]["value"].sum()
    node_stats[node] = (incoming, outgoing)

# è®¡ç®—æ€»èŠ‚ç‚¹çš„æ€»æµå…¥ï¼ˆç”¨äºèŠ‚ç‚¹å æ¯”è®¡ç®—ï¼‰
total_node_values = {
    "æ€»æ›å…‰": node_stats.get("æ€»æ›å…‰", (0, 0))[0],  # æ€»æ›å…‰çš„æ€»æµå…¥
    "æ€»ç‚¹å‡»": node_stats.get("æ€»ç‚¹å‡»", (0, 0))[0],  # æ€»ç‚¹å‡»çš„æ€»æµå…¥
    "æ€»é”€é‡": node_stats.get("æ€»é”€é‡", (0, 0))[0]   # æ€»é”€é‡çš„æ€»æµå…¥
}

# ç”ŸæˆèŠ‚ç‚¹çš„customdataï¼ˆåŒ…å«å æ¯”ï¼‰
node_customdata = []
for node in all_nodes:
    incoming = node_stats[node][0]
    outgoing = node_stats[node][1]
    ratio = ""
    
    # æ’é™¤æ€»æ›å…‰ã€æ€»ç‚¹å‡»ã€æ€»é”€é‡ï¼Œä¸æ˜¾ç¤ºå®ƒä»¬çš„å æ¯”
    if node in ["æ€»æ›å…‰", "æ€»ç‚¹å‡»", "æ€»é”€é‡"]:
        pass  # è¿™ä¸‰ä¸ªèŠ‚ç‚¹çš„å æ¯”è®¾ä¸ºç©º
    else:
        # åˆ¤æ–­èŠ‚ç‚¹ç±»å‹ï¼Œè®¡ç®—å å¯¹åº”æ€»èŠ‚ç‚¹çš„æ¯”ä¾‹
        if "æ›å…‰" in node:
            total = total_node_values["æ€»æ›å…‰"]
            if total > 0:
                ratio = f"å æ€»æ›å…‰ï¼š{round((outgoing / total) * 100, 2)}%"
        elif "ç‚¹å‡»" in node:
            total = total_node_values["æ€»ç‚¹å‡»"]
            if total > 0:
                ratio = f"å æ€»ç‚¹å‡»ï¼š{round((outgoing / total) * 100, 2)}%"
        elif "é”€é‡" in node:
            total = total_node_values["æ€»é”€é‡"]
            if total > 0:
                ratio = f"å æ€»é”€é‡ï¼š{round((outgoing / total) * 100, 2)}%"
    
    node_customdata.append((incoming, outgoing, ratio))

# ===================== 10. æœç´¢å…³é”®è¯åŒ¹é… =====================
search_keyword = search_keyword.strip().lower() if isinstance(search_keyword, str) else ""
matched_traffic_types = []

if not search_keyword:
    matched_traffic_types = TRAFFIC_ORDER
else:
    matched_sites = []
    for site in SITE_CONFIG:
        if search_keyword in site.lower() or search_keyword in SITE_CONFIG[site]["cn_name"].lower():
            matched_sites.append(site)
    
    if matched_sites:
        matched_traffic_types = [t for t in TRAFFIC_ORDER if TRAFFIC_MAPPING[t]["site"] in matched_sites]
    else:
        matched_traffic_types = [t for t in TRAFFIC_ORDER if search_keyword in t.lower()]

# ç”ŸæˆåŒ¹é…èŠ‚ç‚¹åˆ—è¡¨
matched_nodes = []
for traffic_type in matched_traffic_types:
    cfg = TRAFFIC_MAPPING[traffic_type]
    matched_nodes.extend([
        traffic_type,
        cfg["nodes"]["exposure"],
        cfg["nodes"]["click"],
        cfg["nodes"]["sales"],
        cfg["nodes"]["level2_exposure"],
        cfg["nodes"]["level2_click"],
        cfg["nodes"]["level2_sales"]
    ])
matched_nodes = list(set(matched_nodes))

# ===================== 11. ç”Ÿæˆé“¾è·¯ =====================
total_incoming = aggregated_df.groupby("target")["value"].sum().to_dict()
exposure_link = [
    (s, TRAFFIC_MAPPING[s]["nodes"]["exposure"]) for s in TRAFFIC_ORDER
] + [
    (TRAFFIC_MAPPING[s]["nodes"]["exposure"], TRAFFIC_MAPPING[s]["nodes"]["level2_exposure"]) for s in TRAFFIC_ORDER
] + [
    (TRAFFIC_MAPPING[s]["nodes"]["level2_exposure"], "æ€»æ›å…‰") for s in TRAFFIC_ORDER
]

link_sources = []
link_targets = []
link_values = []
link_customdata = []
link_colors = []

for _, row in aggregated_df.iterrows():
    source = row["source"]
    target = row["target"]
    original_val = row["value"]
    group = row["group"]
    traffic_type = row["traffic_type"]
    
    is_matched = traffic_type in matched_traffic_types
    is_exposure = (source, target) in exposure_link
    base_scaled_val = original_val * (exposure_scale if is_exposure else later_scale)
    final_val = base_scaled_val if is_matched else base_scaled_val * 0.05
    
    # æ ¸å¿ƒï¼šç™¾åˆ†æ¯”è®¡ç®—ï¼ˆä¿ç•™2ä½å°æ•°ï¼‰
    target_total = total_incoming.get(target, 1)
    ratio = round((original_val / target_total) * 100, 2)
    
    final_color = GROUP_COLORS[group] if is_matched else "rgba(200, 200, 200, 0.2)"
    
    link_sources.append(node_ids[source])
    link_targets.append(node_ids[target])
    link_values.append(final_val)
    link_colors.append(final_color)
    link_customdata.append([source, target, original_val, ratio])

# ===================== 12. èŠ‚ç‚¹é¢œè‰² =====================
node_color_list = []
for node in all_nodes:
    if node in matched_nodes:
        if node in NODE_TO_TRAFFIC:
            traffic_type = NODE_TO_TRAFFIC[node]
            node_color = GROUP_COLORS[TRAFFIC_MAPPING[traffic_type]["group_id"]]
        else:
            node_color = GROUP_COLORS.get(
                next((site for site in SITE_CONFIG if site in node), "æ€»èŠ‚ç‚¹"),
                "lightgray"
            )
    else:
        node_color = "rgba(200, 200, 200, 0.2)"
    node_color_list.append(node_color)

# ===================== 13. ç»˜åˆ¶æ¡‘åŸºå›¾ =====================
fig = go.Figure(data=[go.Sankey(
    node=dict(
        pad=20,
        thickness=30,
        line=dict(color="black", width=1),
        label=all_nodes,
        color=node_color_list,
        hovertemplate="%{label}<br>æµå…¥ï¼š%{customdata[0]:.0f}<br>æµå‡ºï¼š%{customdata[1]:.0f}<br>%{customdata[2]}<extra></extra>",
        customdata=node_customdata
    ),
    link=dict(
        source=link_sources,
        target=link_targets,
        value=link_values,
        color=link_colors,
        hovertemplate="%{customdata[0]}â†’%{customdata[1]}<br>åŸå§‹æ•°å€¼ï¼š%{customdata[2]:.0f}<br>å %{customdata[1]}æ€»æµå…¥ï¼š%{customdata[3]:.2f}%<extra></extra>",
        customdata=link_customdata
    )
)])

# æ·»åŠ æ ‡é¢˜
title_text = f"å¤šç«™ç‚¹æµé‡è½¬åŒ–è·¯å¾„ï¼ˆ{start_date} è‡³ {end_date}ï¼‰"
if search_keyword:
    title_text += f" | é«˜äº®ï¼š{search_keyword}"

fig.update_layout(
    title_text=title_text,
    font_size=12,
    autosize=True,
    margin=dict(l=20, r=20, t=50, b=20),
    font=dict(family="Microsoft YaHei"),
    height=800
)

# æ˜¾ç¤ºå›¾è¡¨
st.plotly_chart(fig, use_container_width=True, height=800)

# ===================== 14. æ•°æ®æ˜¾ç¤ºåŒºåŸŸ =====================
with st.expander("ğŸ“‹ æŸ¥çœ‹è¯¦ç»†æ•°æ®"):
    tab1, tab2, tab3 = st.tabs(["åŸå§‹æ•°æ®", "æµé‡ç±»å‹ç»Ÿè®¡", "ç«™ç‚¹ç»Ÿè®¡"])
    
    with tab1:
        st.dataframe(filtered_df.head(100))
    
    with tab2:
        # æŒ‰æµé‡ç±»å‹æ±‡æ€»
        traffic_summary = filtered_df.groupby("traffic_type").agg({
            "value": ["sum", "count"]
        }).round(2)
        traffic_summary.columns = ["æ€»æ•°å€¼", "è®°å½•æ•°"]
        st.dataframe(traffic_summary)
    
    with tab3:
        # ç«™ç‚¹ç»Ÿè®¡
        st.write("**ç«™ç‚¹é…ç½®:**")
        for site, info in SITE_CONFIG.items():
            st.write(f"- {site}: {info['cn_name']}")
        
        st.write(f"\n**æµé‡ç±»å‹æ€»æ•°:** {len(TRAFFIC_ORDER)}")
        st.write(f"**åŒ¹é…çš„æµé‡ç±»å‹:** {len(matched_traffic_types)}")

# ===================== 15. é¡µè„šä¿¡æ¯ =====================
st.markdown("---")
st.caption(f"ğŸ“… æ•°æ®æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
st.caption("ğŸ’¡ æç¤ºï¼šä¿®æ”¹Excelæ–‡ä»¶åï¼Œé‡æ–°ä¸Šä¼ å³å¯æ›´æ–°å›¾è¡¨")

# ===================== 16. è¿è¡Œåº”ç”¨ =====================
if __name__ == '__main__':
    # åœ¨æœ¬åœ°è¿è¡ŒStreamlitåº”ç”¨
    # å‘½ä»¤è¡Œè¿è¡Œ: streamlit run sankey_traffic_streamlit.py
    pass
