# sankey_traffic_streamlit.py
import pandas as pd
import plotly.graph_objects as go
import logging
import streamlit as st
from datetime import datetime

# ===================== 1. é¡µé¢é…ç½® =====================
st.set_page_config(
    page_title="å¤šç«™ç‚¹æµé‡-é”€é‡æ¡‘åŸºå›¾åˆ†æ",
    page_icon="ğŸŒ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ===================== 2. å…¨å±€é…ç½® =====================
SITE_CONFIG = {
    "Amazon-US": {"cn_name": "äºšé©¬é€Šç¾å›½ç«™", "color": "#87CEEB"},
    "Amazon-JP": {"cn_name": "äºšé©¬é€Šæ—¥æœ¬ç«™", "color": "#FF6B6B"},
    "Amazon-UK": {"cn_name": "äºšé©¬é€Šè‹±å›½ç«™", "color": "#4ECDC4"},
    "Shopify": {"cn_name": "Shopifyç‹¬ç«‹ç«™", "color": "#DDA0DD"}
}

# æµé‡ç±»å‹æ’åºï¼ˆåŒ…å«Amazoné¡µé¢æ€»ç‚¹å‡»ï¼Œä¸Excelå€¼å®Œå…¨ä¸€è‡´ï¼‰
TRAFFIC_ORDER = [
    "Amazonç«™å†…å¹¿å‘Š",   # 1
    "Amazon-DSP",       # 2
    "Amazonè‡ªç„¶æµé‡",   # 3
    "Amazon-FB",        # 4
    "Amazoné¡µé¢æ€»ç‚¹å‡»", # 5 ä¸Excelæµé‡ç±»å‹å®Œå…¨åŒ¹é…
    "SP-GG",            # 6
    "SP-FB",            # 7
    "SP-è‡ªç„¶",          # 8
    "SP-å…¶ä»–"           # 9
]

# æµé‡æ˜ å°„ï¼ˆç¡®ä¿Amazoné¡µé¢æ€»ç‚¹å‡»é…ç½®ä¸Excelä¸€è‡´ï¼‰
TRAFFIC_MAPPING = {
    "Amazonç«™å†…å¹¿å‘Š": {
        "group_id": "ç»„1",
        "site": "Amazon-US",
        "nodes": {
            "exposure": "ç«™å†…æ›å…‰",
            "level2_exposure": "Amazon-USæ›å…‰",
            "click": "ç«™å†…ç‚¹å‡»",
            "level2_click": "Amazon-USç‚¹å‡»",
            "sales": "ç«™å†…é”€é‡",
            "level2_sales": "Amazon-USé”€é‡"
        }
    },
    "Amazon-DSP": {
        "group_id": "ç»„2",
        "site": "Amazon-US",
        "nodes": {
            "exposure": "DSPæ›å…‰",
            "level2_exposure": "Amazon-USæ›å…‰",
            "click": "DSPç‚¹å‡»",
            "level2_click": "Amazon-USç‚¹å‡»",
            "sales": "DSPé”€é‡",
            "level2_sales": "Amazon-USé”€é‡"
        }
    },
    "Amazonè‡ªç„¶æµé‡": {
        "group_id": "ç»„3",
        "site": "Amazon-US",
        "nodes": {
            "exposure": "Amazonè‡ªç„¶æ›å…‰",
            "level2_exposure": "Amazon-USæ›å…‰",
            "click": "Amazonè‡ªç„¶ç‚¹å‡»",
            "level2_click": "Amazon-USç‚¹å‡»",
            "sales": "Amazonè‡ªç„¶é”€é‡",
            "level2_sales": "Amazon-USé”€é‡"
        }
    },
    "Amazon-FB": {
        "group_id": "ç»„4",
        "site": "Amazon-US",
        "nodes": {
            "exposure": "FBæ›å…‰",
            "level2_exposure": "Amazon-USæ›å…‰",
            "click": "FBç‚¹å‡»",
            "level2_click": "Amazon-USç‚¹å‡»",
            "sales": "FBé”€é‡",
            "level2_sales": "Amazon-USé”€é‡"
        }
    },
    # æ ¸å¿ƒä¿®æ”¹ï¼šAmazoné¡µé¢æ€»ç‚¹å‡»é…ç½®ï¼ˆkeyä¸Excelæµé‡ç±»å‹å®Œå…¨ä¸€è‡´ï¼‰
    "Amazoné¡µé¢æ€»ç‚¹å‡»": {
        "group_id": "ç»„5",
        "site": "Amazon-US",
        "nodes": {
            "exposure": "Amazoné¡µé¢æ€»ç‚¹å‡»æ›å…‰",
            "level2_exposure": "Amazon-USæ›å…‰",
            "click": "Amazoné¡µé¢æ€»ç‚¹å‡»",
            "level2_click": "Amazon-USç‚¹å‡»",
            "sales": "Amazoné¡µé¢æ€»ç‚¹å‡»é”€é‡",
            "level2_sales": "Amazon-USé”€é‡"
        }
    },
    "SP-GG": {
        "group_id": "ç»„6",
        "site": "Shopify",
        "nodes": {
            "exposure": "SP-GGæ›å…‰",
            "level2_exposure": "Shopifyæ›å…‰",
            "click": "SP-GGç‚¹å‡»",
            "level2_click": "Shopifyç‚¹å‡»",
            "sales": "SP-GGé”€é‡",
            "level2_sales": "Shopifyé”€é‡"
        }
    },
    "SP-FB": {
        "group_id": "ç»„7",
        "site": "Shopify",
        "nodes": {
            "exposure": "SP-FBæ›å…‰",
            "level2_exposure": "Shopifyæ›å…‰",
            "click": "SP-FBç‚¹å‡»",
            "level2_click": "Shopifyç‚¹å‡»",
            "sales": "SP-FBé”€é‡",
            "level2_sales": "Shopifyé”€é‡"
        }
    },
    "SP-è‡ªç„¶": {
        "group_id": "ç»„8",
        "site": "Shopify",
        "nodes": {
            "exposure": "SP-è‡ªç„¶æ›å…‰",
            "level2_exposure": "Shopifyæ›å…‰",
            "click": "SP-è‡ªç„¶ç‚¹å‡»",
            "level2_click": "Shopifyç‚¹å‡»",
            "sales": "SP-è‡ªç„¶é”€é‡",
            "level2_sales": "Shopifyé”€é‡"
        }
    },
    "SP-å…¶ä»–": {
        "group_id": "ç»„9",
        "site": "Shopify",
        "nodes": {
            "exposure": "SP-å…¶ä»–æ›å…‰",
            "level2_exposure": "Shopifyæ›å…‰",
            "click": "SP-å…¶ä»–ç‚¹å‡»",
            "level2_click": "Shopifyç‚¹å‡»",
            "sales": "SP-å…¶ä»–é”€é‡",
            "level2_sales": "Shopifyé”€é‡"
        }
    }
}

# åˆ†ç»„é¢œè‰²ï¼ˆåŒ…å«ç»„5ï¼šAmazoné¡µé¢æ€»ç‚¹å‡»ï¼‰
GROUP_COLORS = {
    "ç»„1": "#9290E6",  # ç«™å†…å¹¿å‘Š
    "ç»„2": "#4ECDC4",  # DSP
    "ç»„3": "#45B7D1",  # è‡ªç„¶æµé‡
    "ç»„4": "#96CEB4",  # FB
    "ç»„5": "#6FA8DC",  # Amazoné¡µé¢æ€»ç‚¹å‡»ï¼ˆè“è‰²ç³»åŒ¹é…Amazonç«™ç‚¹ï¼‰
    "ç»„6": "#FFA726",  # SP-GG
    "ç»„7": "#AB47BC",  # SP-FB
    "ç»„8": "#1C363F",  # SP-è‡ªç„¶
    "ç»„9": "#F00B0B",  # SP-å…¶ä»–
    **{site: SITE_CONFIG[site]["color"] for site in SITE_CONFIG},
    "æ€»èŠ‚ç‚¹": "lightgray"
}

# åŠ¨æ€ç”Ÿæˆlevel2èŠ‚ç‚¹åˆ—è¡¨
LEVEL2_NODES = []
for traffic_type in TRAFFIC_MAPPING:
    cfg = TRAFFIC_MAPPING[traffic_type]
    LEVEL2_NODES.extend([
        cfg["nodes"]["level2_exposure"],
        cfg["nodes"]["level2_click"],
        cfg["nodes"]["level2_sales"]
    ])
LEVEL2_NODES = list(set(LEVEL2_NODES))

# èŠ‚ç‚¹â†’æµé‡ç±»å‹æ˜ å°„
NODE_TO_TRAFFIC = {}
for traffic_type in TRAFFIC_MAPPING:
    cfg = TRAFFIC_MAPPING[traffic_type]
    unique_nodes = [
        traffic_type,
        cfg["nodes"]["exposure"],
        cfg["nodes"]["click"],
        cfg["nodes"]["sales"]
    ]
    for node in unique_nodes:
        NODE_TO_TRAFFIC[node] = traffic_type

# æ— æ•ˆæµé‡ç±»å‹è¿‡æ»¤åˆ—è¡¨ï¼ˆå·²ç§»é™¤Amazoné¡µé¢æ€»ç‚¹å‡»ï¼‰
INVALID_TRAFFIC_TYPES = ["æ€»æ›å…‰", "æ€»ç‚¹å‡»", "æ€»é”€é‡"]

# ===================== 3. è¯»å–Excelå‡½æ•°ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šå…¼å®¹æ•°å€¼ç©ºå€¼+æ˜¾ç¤ºæ’æŸ¥æ—¥å¿—ï¼‰ =====================
@st.cache_data
def read_excel_generate_data(excel_path):
    try:
        df = pd.read_excel(excel_path)
        logger.info(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œæ•°æ®è¡Œæ•°ï¼š{len(df)}")
        # æ–°å¢ï¼šæ˜¾ç¤ºExcelåˆ—åå’Œå‰2è¡Œï¼Œæ–¹ä¾¿æ’æŸ¥åˆ—ååŒ¹é…é—®é¢˜
        st.success(f"âœ… æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œæ•°æ®è¡Œæ•°ï¼š{len(df)}")
        st.info(f"ğŸ“‹ Excelåˆ—åï¼š{', '.join(df.columns.tolist())}")
        st.info("ğŸ” Excelå‰2è¡Œæ•°æ®é¢„è§ˆï¼š")
        st.dataframe(df.head(2).style.set_caption("Excelæ•°æ®æ ¼å¼éªŒè¯"))
        
    except Exception as e:
        logger.error(f"è¯»å–Excelå¤±è´¥ï¼š{str(e)}")
        st.error(f"âŒ è¯»å–Excelå¤±è´¥ï¼š{str(e)}")
        return pd.DataFrame()
    
    # æ•°æ®é¢„å¤„ç†ï¼šæ—¶é—´åˆ—å¤„ç†
    if "æ—¶é—´" not in df.columns:
        st.error("âŒ Excelç¼ºå°‘ã€Œæ—¶é—´ã€åˆ—ï¼Œè¯·æ£€æŸ¥åˆ—å")
        return pd.DataFrame()
    
    df["æ—¶é—´_str"] = df["æ—¶é—´"].astype(str)
    df["date"] = df["æ—¶é—´_str"].str.split(" ").str[0].str.replace("/", "-")
    df["date"] = df["date"].replace(["nan", "NaT", ""], pd.NaT)
    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    
    # æ–°å¢ï¼šæ˜¾ç¤ºæ—¶é—´åˆ—å¤„ç†ç»“æœï¼Œæ’æŸ¥æ—¥æœŸæ ¼å¼é—®é¢˜
    valid_date_count = df["date"].notna().sum()
    st.info(f"ğŸ“… æ—¶é—´åˆ—å¤„ç†ç»“æœï¼šæœ‰æ•ˆæ—¥æœŸè¡Œæ•°{valid_date_count} / æ€»è¡Œæ•°{len(df)}")
    if valid_date_count == 0:
        st.warning("âš ï¸ æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ—¥æœŸï¼Œè¯·æ£€æŸ¥Excelã€Œæ—¶é—´ã€åˆ—æ ¼å¼ï¼ˆå»ºè®®æ ¼å¼ï¼š2026/1/5ï¼‰")

    # æ£€æŸ¥æ ¸å¿ƒæ•°å€¼åˆ—æ˜¯å¦å­˜åœ¨
    required_cols = ["æµé‡ç±»å‹", "æ›å…‰", "ç‚¹å‡»", "é”€é‡"]
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        st.error(f"âŒ Excelç¼ºå°‘æ ¸å¿ƒåˆ—ï¼š{', '.join(missing_cols)}ï¼Œè¯·è¡¥å……åé‡æ–°ä¸Šä¼ ")
        return pd.DataFrame()

    data_raw = []
    skipped_count = 0  # ç»Ÿè®¡è¢«è·³è¿‡çš„è¡Œæ•°
    for idx, row in df.iterrows():
        # 1. è¿‡æ»¤ç©ºæ—¥æœŸ
        if pd.isna(row["date"]):
            skipped_count +=1
            continue
        
        # 2. è¿‡æ»¤æ— æ•ˆæµé‡ç±»å‹
        traffic_type = str(row["æµé‡ç±»å‹"]).strip()  # å»é™¤ç©ºæ ¼ï¼Œé¿å…åŒ¹é…å¤±è´¥
        if traffic_type in INVALID_TRAFFIC_TYPES:
            skipped_count +=1
            continue
        
        # 3. è¿‡æ»¤æœªé…ç½®çš„æµé‡ç±»å‹ï¼ˆå¹¶æç¤ºå…·ä½“å€¼ï¼‰
        if traffic_type not in TRAFFIC_MAPPING:
            skipped_count +=1
            st.warning(f"âš ï¸ ç¬¬{idx+1}è¡Œï¼šæµé‡ç±»å‹ã€Œ{traffic_type}ã€æœªåœ¨ä»£ç ä¸­é…ç½®ï¼Œå·²è·³è¿‡")
            continue
        
        # 4. è¿‡æ»¤éæ³•ç«™ç‚¹
        cfg = TRAFFIC_MAPPING[traffic_type]
        if cfg["site"] not in SITE_CONFIG:
            skipped_count +=1
            continue
        
        # æ ¸å¿ƒä¿®æ”¹ï¼šå…¼å®¹æ•°å€¼åˆ—ç©ºå€¼/Noneï¼ˆå°†Noneã€ç©ºå­—ç¬¦ä¸²è½¬ä¸º0.0ï¼‰
        exposure = pd.to_numeric(row["æ›å…‰"], errors="coerce") if (pd.notna(row["æ›å…‰"]) and str(row["æ›å…‰"]).strip() != "") else 0.0
        click = pd.to_numeric(row["ç‚¹å‡»"], errors="coerce") if (pd.notna(row["ç‚¹å‡»"]) and str(row["ç‚¹å‡»"]).strip() != "") else 0.0
        sales = pd.to_numeric(row["é”€é‡"], errors="coerce") if (pd.notna(row["é”€é‡"]) and str(row["é”€é‡"]).strip() != "") else 0.0
        date = row["date"].strftime("%Y-%m-%d")
        
        # ç”Ÿæˆé“¾è·¯æ•°æ®
        data_raw.extend([
            [traffic_type, cfg["nodes"]["exposure"], float(exposure), date, cfg["group_id"], traffic_type],
            [cfg["nodes"]["exposure"], cfg["nodes"]["level2_exposure"], float(exposure), date, cfg["group_id"], traffic_type],
            [cfg["nodes"]["level2_exposure"], "æ€»æ›å…‰", float(exposure), date, cfg["group_id"], traffic_type],
            ["æ€»æ›å…‰", cfg["nodes"]["click"], float(click), date, cfg["group_id"], traffic_type],
            [cfg["nodes"]["click"], cfg["nodes"]["level2_click"], float(click), date, cfg["group_id"], traffic_type],
            [cfg["nodes"]["level2_click"], "æ€»ç‚¹å‡»", float(click), date, cfg["group_id"], traffic_type],
            ["æ€»ç‚¹å‡»", cfg["nodes"]["sales"], float(sales), date, cfg["group_id"], traffic_type],
            [cfg["nodes"]["sales"], cfg["nodes"]["level2_sales"], float(sales), date, cfg["group_id"], traffic_type],
            [cfg["nodes"]["level2_sales"], "æ€»é”€é‡", float(sales), date, cfg["group_id"], traffic_type]
        ])
    
    # æ˜¾ç¤ºæ•°æ®è¿‡æ»¤ç»Ÿè®¡ï¼Œæ–¹ä¾¿æ’æŸ¥
    st.info(f"ğŸ” æ•°æ®è¿‡æ»¤ç»Ÿè®¡ï¼šæ€»è¡Œæ•°{len(df)} â†’ è·³è¿‡{skipped_count}è¡Œ â†’ æœ‰æ•ˆé“¾è·¯æ•°æ®{len(data_raw)}æ¡")
    if len(data_raw) == 0:
        st.error("âŒ æœªç”Ÿæˆæœ‰æ•ˆé“¾è·¯æ•°æ®ï¼Œè¯·æ ¹æ®ä¸Šè¿°æç¤ºæ£€æŸ¥Excelå†…å®¹")
        return pd.DataFrame()
    
    # æœ€ç»ˆæ•°æ®å¤„ç†
    result_df = pd.DataFrame(data_raw, columns=["source", "target", "value", "date", "group", "traffic_type"])
    result_df["date"] = pd.to_datetime(result_df["date"])
    result_df["value"] = pd.to_numeric(result_df["value"], errors="coerce").fillna(0.0)
    logger.info(f"ç”Ÿæˆé“¾è·¯æ•°æ®æ¡æ•°ï¼š{len(result_df)}")
    return result_df

# ===================== 4. åº”ç”¨æ ‡é¢˜ =====================
st.title("ğŸŒ å¤šç«™ç‚¹æµé‡-é”€é‡æ¡‘åŸºå›¾åˆ†æ")
st.markdown("---")

# ===================== 5. æ–‡ä»¶ä¸Šä¼ å’Œæ•°æ®åŠ è½½ =====================
default_excel_path = "1.5-1.19æµé‡æ•°æ®ç»Ÿè®¡.xlsx"
df = pd.DataFrame()

with st.sidebar:
    st.header("âš™ï¸ æ§åˆ¶é¢æ¿")
    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader("ä¸Šä¼ Excelæ–‡ä»¶", type=["xlsx", "xls"])

# ç¡®å®šExcelæ–‡ä»¶è·¯å¾„å¹¶åŠ è½½æ•°æ®
if uploaded_file is not None:
    EXCEL_PATH = uploaded_file
    df = read_excel_generate_data(EXCEL_PATH)
    st.sidebar.success(f"ğŸ“‚ å·²ä¸Šä¼ æ–‡ä»¶: {uploaded_file.name}")
else:
    # æœ¬åœ°æµ‹è¯•é»˜è®¤æ–‡ä»¶
    try:
        df = read_excel_generate_data(default_excel_path)
        st.sidebar.info(f"ğŸ“‚ ä½¿ç”¨é»˜è®¤æ–‡ä»¶: {default_excel_path}")
    except Exception as e:
        st.sidebar.error(f"âŒ é»˜è®¤æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")

# æå–Excelä¸­çš„å®é™…æœ‰æ•ˆæ—¥æœŸèŒƒå›´
default_start_date = datetime.strptime("2026-01-05", "%Y-%m-%d").date()
default_end_date = datetime.strptime("2026-01-19", "%Y-%m-%d").date()

if not df.empty and df["date"].notna().any():
    min_date = df["date"].min()
    max_date = df["date"].max()
    default_start_date = min_date.date()
    default_end_date = max_date.date()
    logger.info(f"è‡ªåŠ¨æå–Excelæ—¥æœŸèŒƒå›´ï¼š{default_start_date} è‡³ {default_end_date}")
else:
    logger.warning("æœªæå–åˆ°æœ‰æ•ˆæ—¥æœŸï¼Œä½¿ç”¨å…œåº•é»˜è®¤å€¼")

# ===================== 6. ä¾§è¾¹æ å…¶ä»–æ§ä»¶ =====================
with st.sidebar:
    # æœç´¢åŒºåŸŸï¼ˆæ”¯æŒæœç´¢Amazoné¡µé¢æ€»ç‚¹å‡»ï¼‰
    search_keyword = st.text_input(
        "ğŸ” é“¾è·¯æœç´¢ï¼ˆæ”¯æŒç«™ç‚¹/æµé‡ç±»å‹å…³é”®è¯ï¼‰",
        placeholder="è¾“å…¥å…³é”®è¯ï¼ˆå¦‚é¡µé¢æ€»ç‚¹å‡»/US/Shopifyï¼‰",
        help="æ”¯æŒæœç´¢ã€Œé¡µé¢æ€»ç‚¹å‡»ã€å¿«é€Ÿå®šä½æ–°å¢é“¾è·¯"
    )
    
    # æ¸…ç©ºæœç´¢æŒ‰é’®
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæœç´¢", type="secondary", use_container_width=True):
            search_keyword = ""
            st.rerun()
    
    st.markdown("---")
    st.subheader("ğŸ“… æ—¥æœŸèŒƒå›´")
    
    # æ—¥æœŸè¾“å…¥
    col1, col2 = st.columns(2)
    with col1:
        start_date = st.date_input(
            "å¼€å§‹æ—¥æœŸ",
            value=default_start_date,
            help="é»˜è®¤æ˜¾ç¤ºExcelä¸­çš„æœ€æ—©æ—¥æœŸ"
        )
    
    with col2:
        end_date = st.date_input(
            "ç»“æŸæ—¥æœŸ",
            value=default_end_date,
            help="é»˜è®¤æ˜¾ç¤ºExcelä¸­çš„æœ€æ™šæ—¥æœŸ"
        )
    
    # æ—¥æœŸéªŒè¯
    if start_date > end_date:
        st.warning("âš ï¸ å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸï¼Œå·²è‡ªåŠ¨äº¤æ¢")
        start_date, end_date = end_date, start_date
    
    st.markdown("---")
    st.subheader("ğŸ“ ç¼©æ”¾æ§åˆ¶")
    
    # ç¼©æ”¾ç³»æ•°
    col1, col2 = st.columns(2)
    with col1:
        exposure_scale = st.number_input(
            "æ›å…‰é“¾è·¯ç¼©æ”¾",
            min_value=0.01,
            max_value=10.0,
            value=0.5,
            step=0.05,
            help="è°ƒæ•´æ›å…‰é“¾è·¯çš„å®½åº¦"
        )
    
    with col2:
        later_scale = st.number_input(
            "åç»­é“¾è·¯ç¼©æ”¾",
            min_value=0.01,
            max_value=50.0,
            value=5.0,
            step=1.0,
            help="è°ƒæ•´ç‚¹å‡»å’Œé”€é‡é“¾è·¯çš„å®½åº¦"
        )
    
    st.markdown("---")
    st.info("ğŸ’¡ æç¤ºï¼šç‚¹å‡»å›¾è¡¨èŠ‚ç‚¹å¯æŸ¥çœ‹æµå…¥/æµå‡ºæ•°æ®åŠå æ¯”")

# ===================== 7. æ•°æ®éªŒè¯ =====================
if df.empty:
    st.error("âŒ æ— æœ‰æ•ˆæ•°æ®å¯å±•ç¤ºï¼Œè¯·æ ¹æ®ä¸Šæ–¹æç¤ºæ£€æŸ¥Excelæ–‡ä»¶åé‡æ–°ä¸Šä¼ ")
    st.stop()

# ===================== 8. æ•°æ®ç­›é€‰å’Œå¤„ç† =====================
# æ˜¾ç¤ºæ•°æ®æ‘˜è¦
with st.expander("ğŸ“Š æ•°æ®æ‘˜è¦", expanded=True):
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        total_records = len(df)
        st.metric("æ€»é“¾è·¯è®°å½•æ•°", total_records)
    
    with col2:
        traffic_types = df["traffic_type"].nunique()
        st.metric("æœ‰æ•ˆæµé‡ç±»å‹æ•°", traffic_types)
    
    with col3:
        total_exposure = df[df["source"].str.contains("æ›å…‰")]["value"].sum()
        st.metric("æ€»æ›å…‰é‡", f"{total_exposure:,.0f}")
    
    with col4:
        total_sales = df[df["source"].str.contains("é”€é‡")]["value"].sum()
        st.metric("æ€»é”€é‡", f"{total_sales:,.0f}")

# æ•°æ®ç­›é€‰èšåˆ
start_date_dt = pd.Timestamp(start_date)
end_date_dt = pd.Timestamp(end_date)
filtered_df = df[(df["date"] >= start_date_dt) & (df["date"] <= end_date_dt)]
aggregated_df = filtered_df.groupby(["source", "target", "group", "traffic_type"], as_index=False)["value"].sum()
aggregated_df = aggregated_df[aggregated_df["value"] > 0]

# ===================== 9. ç”ŸæˆèŠ‚ç‚¹åˆ—è¡¨ =====================
# æ‹†åˆ†Amazonå’ŒShopifyæµé‡ç±»å‹
Amazon_TRAFFIC = [t for t in TRAFFIC_ORDER if TRAFFIC_MAPPING[t]["site"] == "Amazon-US"]
Shopify_TRAFFIC = [t for t in TRAFFIC_ORDER if TRAFFIC_MAPPING[t]["site"] == "Shopify"]

# Amazonç»„èŠ‚ç‚¹ï¼ˆåŒ…å«é¡µé¢æ€»ç‚¹å‡»ï¼‰
Amazon_flow_sources = Amazon_TRAFFIC
Amazon_exposure_nodes = [TRAFFIC_MAPPING[t]["nodes"]["exposure"] for t in Amazon_TRAFFIC]
Amazon_level2_exposure = list(set([TRAFFIC_MAPPING[t]["nodes"]["level2_exposure"] for t in Amazon_TRAFFIC]))
Amazon_click_nodes = [TRAFFIC_MAPPING[t]["nodes"]["click"] for t in Amazon_TRAFFIC]
Amazon_level2_click = list(set([TRAFFIC_MAPPING[t]["nodes"]["level2_click"] for t in Amazon_TRAFFIC]))
Amazon_sales_nodes = [TRAFFIC_MAPPING[t]["nodes"]["sales"] for t in Amazon_TRAFFIC]
Amazon_level2_sales = list(set([TRAFFIC_MAPPING[t]["nodes"]["level2_sales"] for t in Amazon_TRAFFIC]))

# Shopifyç»„èŠ‚ç‚¹
Shopify_flow_sources = Shopify_TRAFFIC
Shopify_exposure_nodes = [TRAFFIC_MAPPING[t]["nodes"]["exposure"] for t in Shopify_TRAFFIC]
Shopify_level2_exposure = list(set([TRAFFIC_MAPPING[t]["nodes"]["level2_exposure"] for t in Shopify_TRAFFIC]))
Shopify_click_nodes = [TRAFFIC_MAPPING[t]["nodes"]["click"] for t in Shopify_TRAFFIC]
Shopify_level2_click = list(set([TRAFFIC_MAPPING[t]["nodes"]["level2_click"] for t in Shopify_TRAFFIC]))
Shopify_sales_nodes = [TRAFFIC_MAPPING[t]["nodes"]["sales"] for t in Shopify_TRAFFIC]
Shopify_level2_sales = list(set([TRAFFIC_MAPPING[t]["nodes"]["level2_sales"] for t in Shopify_TRAFFIC]))

# æ€»èŠ‚ç‚¹
total_nodes = ["æ€»æ›å…‰", "æ€»ç‚¹å‡»", "æ€»é”€é‡"]

# æ‹¼æ¥æ‰€æœ‰èŠ‚ç‚¹ï¼ˆAmazonåœ¨å‰ï¼ŒShopifyåœ¨åï¼‰
all_nodes = (
    Amazon_flow_sources + Amazon_exposure_nodes + Amazon_level2_exposure + 
    total_nodes[:1] +  # æ€»æ›å…‰
    Amazon_click_nodes + Amazon_level2_click + 
    total_nodes[1:2] +  # æ€»ç‚¹å‡»
    Amazon_sales_nodes + Amazon_level2_sales + 
    Shopify_flow_sources + Shopify_exposure_nodes + Shopify_level2_exposure + 
    Shopify_click_nodes + Shopify_level2_click + 
    Shopify_sales_nodes + Shopify_level2_sales + 
    total_nodes[2:]  # æ€»é”€é‡
)

node_ids = {node: idx for idx, node in enumerate(all_nodes)}

# ===================== 10. èŠ‚ç‚¹ç»Ÿè®¡ =====================
node_stats = {}
for node in all_nodes:
    incoming = aggregated_df[aggregated_df["target"] == node]["value"].sum()
    outgoing = aggregated_df[aggregated_df["source"] == node]["value"].sum()
    node_stats[node] = (incoming, outgoing)

# æ€»èŠ‚ç‚¹æ•°å€¼ï¼ˆç”¨äºè®¡ç®—å æ¯”ï¼‰
total_node_values = {
    "æ€»æ›å…‰": node_stats.get("æ€»æ›å…‰", (0, 0))[0],
    "æ€»ç‚¹å‡»": node_stats.get("æ€»ç‚¹å‡»", (0, 0))[0],
    "æ€»é”€é‡": node_stats.get("æ€»é”€é‡", (0, 0))[0]
}

# èŠ‚ç‚¹è‡ªå®šä¹‰æ•°æ®ï¼ˆåŒ…å«å æ¯”ï¼‰
node_customdata = []
for node in all_nodes:
    incoming = node_stats[node][0]
    outgoing = node_stats[node][1]
    ratio = ""
    
    if node not in ["æ€»æ›å…‰", "æ€»ç‚¹å‡»", "æ€»é”€é‡"]:
        if "æ›å…‰" in node and total_node_values["æ€»æ›å…‰"] > 0:
            ratio = f"å æ€»æ›å…‰ï¼š{round((outgoing / total_node_values['æ€»æ›å…‰']) * 100, 2)}%"
        elif "ç‚¹å‡»" in node and total_node_values["æ€»ç‚¹å‡»"] > 0:
            ratio = f"å æ€»ç‚¹å‡»ï¼š{round((outgoing / total_node_values['æ€»ç‚¹å‡»']) * 100, 2)}%"
        elif "é”€é‡" in node and total_node_values["æ€»é”€é‡"] > 0:
            ratio = f"å æ€»é”€é‡ï¼š{round((outgoing / total_node_values['æ€»é”€é‡']) * 100, 2)}%"
    
    node_customdata.append((incoming, outgoing, ratio))

# ===================== 11. æœç´¢å…³é”®è¯åŒ¹é… =====================
search_keyword = search_keyword.strip().lower() if isinstance(search_keyword, str) else ""
matched_traffic_types = []

if not search_keyword:
    matched_traffic_types = TRAFFIC_ORDER
else:
    # åŒ¹é…ç«™ç‚¹æˆ–æµé‡ç±»å‹
    matched_sites = [site for site in SITE_CONFIG if search_keyword in site.lower() or search_keyword in SITE_CONFIG[site]["cn_name"].lower()]
    if matched_sites:
        matched_traffic_types = [t for t in TRAFFIC_ORDER if TRAFFIC_MAPPING[t]["site"] in matched_sites]
    else:
        matched_traffic_types = [t for t in TRAFFIC_ORDER if search_keyword in t.lower()]

# åŒ¹é…èŠ‚ç‚¹åˆ—è¡¨
matched_nodes = []
for traffic_type in matched_traffic_types:
    cfg = TRAFFIC_MAPPING[traffic_type]
    matched_nodes.extend([
        traffic_type,
        cfg["nodes"]["exposure"],
        cfg["nodes"]["click"],
        cfg["nodes"]["sales"],
        cfg["nodes"]["level2_exposure"],
        cfg["nodes"]["level2_click"],
        cfg["nodes"]["level2_sales"]
    ])
matched_nodes = list(set(matched_nodes))

# ===================== 12. ç”Ÿæˆé“¾è·¯ =====================
total_incoming = aggregated_df.groupby("target")["value"].sum().to_dict()
exposure_link = [
    (s, TRAFFIC_MAPPING[s]["nodes"]["exposure"]) for s in TRAFFIC_ORDER
] + [
    (TRAFFIC_MAPPING[s]["nodes"]["exposure"], TRAFFIC_MAPPING[s]["nodes"]["level2_exposure"]) for s in TRAFFIC_ORDER
] + [
    (TRAFFIC_MAPPING[s]["nodes"]["level2_exposure"], "æ€»æ›å…‰") for s in TRAFFIC_ORDER
]

link_sources = []
link_targets = []
link_values = []
link_customdata = []
link_colors = []

for _, row in aggregated_df.iterrows():
    source = row["source"]
    target = row["target"]
    original_val = row["value"]
    group = row["group"]
    traffic_type = row["traffic_type"]
    
    # é“¾è·¯åŒ¹é…ä¸ç¼©æ”¾
    is_matched = traffic_type in matched_traffic_types
    is_exposure = (source, target) in exposure_link
    base_scaled_val = original_val * (exposure_scale if is_exposure else later_scale)
    final_val = base_scaled_val if is_matched else base_scaled_val * 0.05
    
    # è®¡ç®—å æ¯”
    target_total = total_incoming.get(target, 1)
    ratio = round((original_val / target_total) * 100, 2)
    
    # é“¾è·¯é¢œè‰²
    final_color = GROUP_COLORS[group] if is_matched else "rgba(200, 200, 200, 0.2)"
    
    # æ”¶é›†é“¾è·¯æ•°æ®
    link_sources.append(node_ids[source])
    link_targets.append(node_ids[target])
    link_values.append(final_val)
    link_colors.append(final_color)
    link_customdata.append([source, target, original_val, ratio])

# ===================== 13. èŠ‚ç‚¹é¢œè‰² =====================
node_color_list = []
for node in all_nodes:
    if node in matched_nodes:
        if node in NODE_TO_TRAFFIC:
            traffic_type = NODE_TO_TRAFFIC[node]
            node_color = GROUP_COLORS[TRAFFIC_MAPPING[traffic_type]["group_id"]]
        else:
            node_color = GROUP_COLORS.get(
                next((site for site in SITE_CONFIG if site in node), "æ€»èŠ‚ç‚¹"),
                "lightgray"
            )
    else:
        node_color = "rgba(200, 200, 200, 0.2)"
    node_color_list.append(node_color)

# ===================== 14. ç»˜åˆ¶æ¡‘åŸºå›¾ =====================
fig = go.Figure(data=[go.Sankey(
    node=dict(
        pad=20,
        thickness=30,
        line=dict(color="black", width=1),
        label=all_nodes,
        color=node_color_list,
        hovertemplate="%{label}<br>æµå…¥ï¼š%{customdata[0]:.0f}<br>æµå‡ºï¼š%{customdata[1]:.0f}<br>%{customdata[2]}<extra></extra>",
        customdata=node_customdata
    ),
    link=dict(
        source=link_sources,
        target=link_targets,
        value=link_values,
        color=link_colors,
        hovertemplate="%{customdata[0]}â†’%{customdata[1]}<br>åŸå§‹æ•°å€¼ï¼š%{customdata[2]:.0f}<br>å %{customdata[1]}æ€»æµå…¥ï¼š%{customdata[3]:.2f}%<extra></extra>",
        customdata=link_customdata
    )
)])

# å›¾è¡¨æ ‡é¢˜ï¼ˆåŒ…å«æœç´¢å…³é”®è¯ï¼‰
title_text = f"å¤šç«™ç‚¹æµé‡è½¬åŒ–è·¯å¾„ï¼ˆ{start_date} è‡³ {end_date}ï¼‰"
if search_keyword:
    title_text += f" | é«˜äº®ï¼š{search_keyword}"

fig.update_layout(
    title_text=title_text,
    font_size=12,
    autosize=True,
    margin=dict(l=20, r=20, t=50, b=20),
    font=dict(family="Microsoft YaHei"),
    height=800
)

# æ˜¾ç¤ºå›¾è¡¨
st.plotly_chart(fig, use_container_width=True, height=800)

# ===================== 15. æ•°æ®æ˜¾ç¤ºåŒºåŸŸ =====================
with st.expander("ğŸ“‹ æŸ¥çœ‹è¯¦ç»†æ•°æ®", expanded=False):
    tab1, tab2, tab3 = st.tabs(["åŸå§‹é“¾è·¯æ•°æ®", "æµé‡ç±»å‹ç»Ÿè®¡", "ç«™ç‚¹é…ç½®"])
    
    with tab1:
        st.dataframe(filtered_df.head(100).style.set_caption("ç­›é€‰åçš„å‰100æ¡é“¾è·¯æ•°æ®"))
    
    with tab2:
        # æµé‡ç±»å‹æ±‡æ€»ç»Ÿè®¡
        traffic_summary = filtered_df.groupby("traffic_type").agg({
            "value": ["sum", "count"]
        }).round(2)
        traffic_summary.columns = ["æ€»æ•°å€¼", "è®°å½•æ•°"]
        st.dataframe(traffic_summary.style.set_caption("å„æµé‡ç±»å‹æ•°æ®ç»Ÿè®¡"))
    
    with tab3:
        # ç«™ç‚¹é…ç½®ä¸æµé‡ç±»å‹åˆ†å¸ƒ
        st.subheader("ç«™ç‚¹é…ç½®è¯¦æƒ…")
        site_df = pd.DataFrame([
            {"ç«™ç‚¹æ ‡è¯†": site, "ä¸­æ–‡åç§°": info["cn_name"], "é¢œè‰²ä»£ç ": info["color"]}
            for site, info in SITE_CONFIG.items()
        ])
        st.dataframe(site_df)
        
        st.subheader("æµé‡ç±»å‹åˆ†å¸ƒ")
        traffic_dist = pd.DataFrame([
            {"æµé‡ç±»å‹": t, "æ‰€å±ç«™ç‚¹": TRAFFIC_MAPPING[t]["site"], "åˆ†ç»„ID": TRAFFIC_MAPPING[t]["group_id"]}
            for t in TRAFFIC_ORDER
        ])
        st.dataframe(traffic_dist)

# ===================== 16. é¡µè„šä¿¡æ¯ =====================
st.markdown("---")
st.caption(f"ğŸ“… æ•°æ®æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
st.caption("ğŸ’¡ æ“ä½œæç¤ºï¼š1. ä¸Šä¼ Excelåå¯æŸ¥çœ‹æ•°æ®æ’æŸ¥æ—¥å¿—ï¼›2. æœç´¢ã€Œé¡µé¢æ€»ç‚¹å‡»ã€å¯å¿«é€Ÿå®šä½æ–°å¢é“¾è·¯")
